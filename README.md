# NExon

Prediction of novel cassette exons from RNA-Seq data. Developed by Sergey Margasyuk (smargasyuk@gmail.com) and Dmitri Pervouchine (pervouchine@gmail.com).

## Input

The pipeline uses an RNA-Seq BAM alignment and a J6 junction quantification file generated by [pyIPSA](https://github.com/pervouchinelab/pyIPSA) for each sample. The location of input files is described in `config/config.yaml` and `config/samples.csv` configuration files. 

### Sample table

Sample table (`config/samples.csv`by default) is a three-column CSV file describing each RNA-Seq experiment file:
 + path: full path to the BAM file;
 + name: sample alias which was used by pyIPSA;
 + meta: sample metadata for the final table;

```
path,name,meta
{root_dir}/{assembly}/bam/{Sample1}.bam,{Sample1},{sample_meta1}
{root_dir}/{assembly}/bam/{Sample1}.bam,{Sample2},{sample_meta2}
```

### Main configuration file

 Main configuration file (`config/config.yaml` by default) should contain `root_dir` and `assembly` keys defining path to the working directory and `samples` key pointing to the samples table.

### Directory structure

The sample input files should be put into `bam` and `pyIPSA` subdirectories of `{root_dir}/{assembly}`. The pipeline generates files in `stringtie` subdirectory of `{root_dir}/{assembly}`:

```
{root_dir}
└── {assembly}
    ├── bam
    │   ├──{Sample1}.bam
    │   ├──{Sample2}.bam
    │   └── ...
    ├── pyIPSA
    │   └── J6
    │       ├── {Sample1}.J6.gz
    │       ├── {Sample2}.J6.gz
    │       └── ...
    └── stringtie
        ├── ...
        └── S10
           └── Exons_table_{conservation_file}.tsv
```

### Annotation input files

Annotation files used by the pipeline should be put into the `resources/annotation/{assembly}` directory. The following files are required:

+ `{assembly}.annotation.gtf`: transcript annotation in GTF format. The pipeline was tested with GENCODE.
+ `phastCons.{conservation_file}.bed`: conserved elements in BED format. The 5th field should contain the conservation score
+ `exons.bed`: annotated exons in BED format. This file is used to exclude partially annotated cassette exons from the output.


## Output

The pipeline outputs table `S10/Exons_table_{conservation_file}.tsv` that contains information about novel exons detected in each sample. The table has the following fields:

+ `exon_id`, `seqname`, `start`, `end`, `strand`: coordinates of the novel exon
+ `sample_name`: sample name from the `name` field of the sample table
+ `coord_prev`, `coord_next`, `junction_id_l`, `junction_id_r`: coordinates of the flanking introns with the highest split read support in the sample
+ `cov`, `ipsa_min`, `cons_avg`: coverage, split read support and conservation score of the exon
+ `ann_cdf_min`: minimum of three quantiles of exon metrics in the annotated exons metrics eCDFs; may be used for exon quality filtering

## Usage

### Step 1: Obtain a copy of this workflow

[Clone](https://help.github.com/en/articles/cloning-a-repository) this repository to your local system, into the place where you want to perform the data analysis.

    git clone https://github.com/smargasyuk/NExon.git
    cd NExon
    git checkout v0.1.0

### Step 2: Configure workflow

Configure the workflow according to your needs via editing the files in the `config/` folder. Adjust `config.yaml` to configure the workflow execution, and `samples.csv` to specify your sample setup.

### Step 3: Install Snakemake

Install Snakemake using [conda](https://conda.io/projects/conda/en/latest/user-guide/install/index.html):

    # install mamba package manager if you don't have it
    conda install -n base -c conda-forge mamba
    conda create -c bioconda -c conda-forge -n snakemake snakemake

For installation details, see the [instructions in the Snakemake documentation](https://snakemake.readthedocs.io/en/stable/getting_started/installation.html).

### Step 4: Execute workflow

Activate the conda environment:

    conda activate snakemake

Test your configuration by performing a dry-run via

    snakemake --use-conda -n

Execute the workflow locally via

    snakemake --use-conda --cores $N

using `$N` cores or run it in a cluster environment via

    snakemake --use-conda --cluster qsub --jobs 100

or

    snakemake --use-conda --drmaa --jobs 100

See the [Snakemake documentation](https://snakemake.readthedocs.io/en/stable/executable.html) for further details.